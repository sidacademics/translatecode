{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8de6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import contractions\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07373ccc",
   "metadata": {},
   "source": [
    "#### There are three available sources. I am choosing sentence pairs from TED talk transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0e93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39881 entries, 0 to 127606\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   source            39881 non-null  object\n",
      " 1   english_sentence  39881 non-null  object\n",
      " 2   hindi_sentence    39881 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/sidac/Downloads/Hindi_English_Truncated_Corpus.csv\")\n",
    "df=df[df['source']=='ted']\n",
    "df.info()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c92f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03fc88",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "#### English Sentences\n",
    "\n",
    "1. The sentences are converted to lowercase.\n",
    "2. Contractions are expanded. (Ex- can't is changed to can not)\n",
    "3. All the digits are removed.\n",
    "4. Extra whitespaces from start and end of the sentences are removed.\n",
    "5. Multiple whitespaces are removed to keep only one.\n",
    "\n",
    "#### Hindi Sentences\n",
    "1. Punctuations like '।' and ''' are removed which are specific to these Hindi Sentences.\n",
    "2. All the digits are removed.\n",
    "3. All the english alphabets are removed.\n",
    "4. Extra whitespaces from start and end of the sentences are removed.\n",
    "5. Multiple whitespaces are removed to keep only one.\n",
    "6. All the special characters are removed.\n",
    "7. Sentences are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3b3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "def preprocess_data(data):\n",
    "    remove_special= set(string.punctuation)\n",
    "    data['english_sentence']=data['english_sentence'].apply(lambda x: x.lower())\n",
    "    data['english_sentence']=data['english_sentence'].apply(expand_contractions)\n",
    "    data['english_sentence']=data['english_sentence'].apply(lambda x: re.sub(r'[^a-zA-Z ]+', \"\", x))\n",
    "    data['english_sentence']=data['english_sentence'].apply(lambda x: x.strip())\n",
    "    data['english_sentence']=data['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: re.sub(\"'\", \"\", x))\n",
    "    data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: re.sub(\"।\", \"\", x))\n",
    "    data['hindi_sentence']=data['hindi_sentence'].str.replace(\"\\d+\", \"\", regex=True)\n",
    "    data['hindi_sentence']=data['hindi_sentence'].str.replace(r'[a-zA-Z]', '', regex=True)\n",
    "    data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: x.strip())\n",
    "    data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "    data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in remove_special))\n",
    "    data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: unicodedata.normalize('NFKC', x))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad423872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39881 entries, 0 to 127606\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   source            39881 non-null  object\n",
      " 1   english_sentence  39881 non-null  object\n",
      " 2   hindi_sentence    39881 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>i would like to tell you about one such child</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they are bad at no...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ted</td>\n",
       "      <td>and who are we to say even that they are wrong</td>\n",
       "      <td>और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ted</td>\n",
       "      <td>so there is some sort of justice</td>\n",
       "      <td>तो वहाँ न्याय है</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                   english_sentence  \\\n",
       "0     ted  politicians do not have permission to do what ...   \n",
       "1     ted      i would like to tell you about one such child   \n",
       "3     ted  what we really mean is that they are bad at no...   \n",
       "7     ted     and who are we to say even that they are wrong   \n",
       "13    ted                   so there is some sort of justice   \n",
       "\n",
       "                                       hindi_sentence  \n",
       "0   राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...  \n",
       "1   मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी  \n",
       "3      हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "7    और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं  \n",
       "13                                   तो वहाँ न्याय है  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_data(df)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f4463",
   "metadata": {},
   "source": [
    "### Train-Val-Test Split (70:20:10)\n",
    "\n",
    "1. DataFrame is being split in required ratio\n",
    "2. Dataset is converted in the required format for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c75da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 27916\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 7976\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3989\n",
      "    })\n",
      "})\n",
      "{'translation': {'en': 'you think that it is not true of course', 'hi': 'आप ये सोच तै है  मगर ये सच नहीं है'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "val_size = int(len(dataset) * 0.2)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "data_train_test = dataset.train_test_split(test_size=test_size)\n",
    "data_train_validation = data_train_test[\"train\"].train_test_split(test_size=val_size)\n",
    "random.seed(None)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"translation\": [{\"en\": src, \"hi\": tgt} for src, tgt in zip(data_train_validation[\"train\"][\"english_sentence\"], data_train_validation[\"train\"][\"hindi_sentence\"])]}),\n",
    "    \"validation\": Dataset.from_dict({\"translation\": [{\"en\": src, \"hi\": tgt} for src, tgt in zip(data_train_validation[\"test\"][\"english_sentence\"], data_train_validation[\"test\"][\"hindi_sentence\"])]}),\n",
    "    \"test\": Dataset.from_dict({\"translation\": [{\"en\": src, \"hi\": tgt} for src, tgt in zip(data_train_test[\"test\"][\"english_sentence\"], data_train_test[\"test\"][\"hindi_sentence\"])]})\n",
    "})\n",
    "\n",
    "\n",
    "print(raw_datasets)\n",
    "print(raw_datasets[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b824fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': [{'en': 'i have a corresponding milestone to robust human rejuvenation', 'hi': 'मेरे पास मजबूत मानव कायाकल्प से मिलता जुलता मानक है'}, {'en': 'and the plant showers them with pollen', 'hi': 'पौधे उन पर पुष्परेणु की बौछार कर देते है'}, {'en': 'that was like just so super cool', 'hi': 'वह वास्त्व मे बहुत मस्त था'}, {'en': 'to cause there to be a shift in our hearts', 'hi': 'कि यह चीज़ हमारे ह्रदय पर कुछ प्रभाव डाले'}, {'en': 'are ufos alien spaceships or perceptual cognitive mistakes or even fakes', 'hi': 'विदेशी अंतरिक्षयान या अवधारणात्मक संज्ञानात्मक गलतियाँ  या यहाँ तक कि झूठ '}, {'en': 'and when you look at the news through that filter', 'hi': 'और जब समाचारों को ऐसी छलनी से छान कर देखते हैं'}, {'en': 'they fly not with rotating components', 'hi': 'वे घूमने वाले उपकरणों के साथ नहीं उड़'}, {'en': 'next to a hungarian physicist about my age', 'hi': 'मेरी उम्र के एक हंगरी के भौतिक विज्ञानी के साथ'}, {'en': 'is now fairly stable and being managed so that business people', 'hi': 'अब काफ़ी स्थिर है और इसे परबंधित किया जा रहा है ताकि व्यापार के लोगों को'}, {'en': 'and he knew very intuitively', 'hi': 'और उन्हें सहजता से पता था'}]}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"test\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591f32e",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc7f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e27d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"hi\"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360a6a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb50f02d9b147f383a0bfa68d0c7fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidac\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d1322ff14043d39596beee625a77f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a587117e0f4a4287f16a939ca6c6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275be9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'to all the beliefs i hold', 'hi': 'मेरी मान्यताओं पर'}, 'input_ids': [7, 98, 4, 4930, 5556, 1763, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'labels': [334, 53706, 33, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f11e62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 27916\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7976\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3989\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27b11fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sidac\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidac\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4bfa76",
   "metadata": {},
   "source": [
    "### Setting up required arguments for Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb3e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ebc084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cd389",
   "metadata": {},
   "source": [
    "## Defining Metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "650b7cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sidac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sidac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sidac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "metric_bleu = load(\"sacrebleu\")\n",
    "metric_meteor = load(\"meteor\")\n",
    "metric_ter = load(\"ter\")\n",
    "import numpy as np \n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_result = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "    \n",
    "    # Compute METEOR score\n",
    "    meteor_result = metric_meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    meteor_score = meteor_result[\"meteor\"]\n",
    "    \n",
    "    # Compute TER score\n",
    "    ter_result = metric_ter.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    ter_score = ter_result[\"score\"]\n",
    "    \n",
    "    result = {\"bleu\": round(bleu_score, 4), \"meteor\": round(meteor_score, 4), \"ter\": round(ter_score, 4)}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532372f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "301d16f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidac\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d37a20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8725' max='8725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8725/8725 8:00:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.250900</td>\n",
       "      <td>3.052552</td>\n",
       "      <td>14.669000</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>75.223100</td>\n",
       "      <td>9.835757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.881600</td>\n",
       "      <td>2.987682</td>\n",
       "      <td>15.003000</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>74.648400</td>\n",
       "      <td>9.880391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.664200</td>\n",
       "      <td>2.968331</td>\n",
       "      <td>15.244900</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>74.198300</td>\n",
       "      <td>9.864218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.509700</td>\n",
       "      <td>2.966589</td>\n",
       "      <td>15.196100</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>74.332100</td>\n",
       "      <td>9.912738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.438700</td>\n",
       "      <td>2.968678</td>\n",
       "      <td>15.287900</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>74.129900</td>\n",
       "      <td>9.896189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8725, training_loss=2.7686472650653653, metrics={'train_runtime': 28838.5332, 'train_samples_per_second': 4.84, 'train_steps_per_second': 0.303, 'total_flos': 591901468655616.0, 'train_loss': 2.7686472650653653, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df8e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"helsinkiFinalwithMetrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce2d1e",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "131617db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Legumes share resources with nitrogen-fixing bacteria.\"\n",
    "text2 = \"my name is John\"\n",
    "text3 = 'he died'\n",
    "text4  = \"i would like to tell you about one such child\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bee06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
    "    print(inputs)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"helsinkiFinalwithMetrics\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return outputs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c33fe86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5556,  178,  288,    7, 1169,   27,  195,  131,  295, 1075,    0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'मैं आपको ऐसे एक बच्चे के बारे में बताना चाहूँगा'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pred = predict(text4)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0788d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"helsinkiFinalwithMetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f2c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4c687f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'i have a corresponding milestone to robust human rejuvenation', 'hi': 'मेरे पास मजबूत मानव कायाकल्प से मिलता जुलता मानक है'}, 'input_ids': [5556, 55, 19, 12182, 30734, 7, 54326, 804, 2498, 15072, 1345, 14124, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [228, 173, 14204, 3243, 24, 1615, 39207, 12, 1855, 30004, 3877, 5, 0]}\n",
      "tensor([[ 5556,    55,    19, 12182, 30734,     7, 54326,   804,  2498, 15072,\n",
      "          1345, 14124,     0]])\n",
      "मेरे पास मजबूत मानव परिक्षण के बराबर महत्वपूर्ण है\n"
     ]
    }
   ],
   "source": [
    "example=tokenized_datasets[\"test\"][0]\n",
    "print(example)\n",
    "input_ids = [example[\"input_ids\"]]\n",
    "attention_mask = [example[\"attention_mask\"]]\n",
    "print(torch.tensor(input_ids))\n",
    "outputs = model.generate(input_ids=torch.tensor(input_ids), attention_mask=torch.tensor(attention_mask))\n",
    "outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb051f",
   "metadata": {},
   "source": [
    "### Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d4dbdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'that was like just so super cool', 'hi': 'वह वास्त्व मे बहुत मस्त था'}, 'input_ids': [26, 80, 288, 469, 166, 8708, 8810, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'labels': [49, 32245, 1185, 926, 1878, 176, 38075, 82, 0]}\n",
      "[[49, 32245, 1185, 926, 1878, 176, 38075, 82, 0]]\n",
      "tensor([[  26,   80,  288,  469,  166, 8708, 8810,    0]])\n",
      "tensor([[61949,    60,  4828,    58,   470,    82,     0]])\n",
      "BLEU Score on Test Example: 8.7458\n",
      "METEOR Score on Test Dataset: 0.0847\n",
      "TER Score on Test Dataset: 83.3333\n"
     ]
    }
   ],
   "source": [
    "example=tokenized_datasets[\"test\"][2]\n",
    "print(example)\n",
    "input_ids = [example[\"input_ids\"]]\n",
    "attention_mask = [example[\"attention_mask\"]]\n",
    "label= [example[\"labels\"]]\n",
    "print(label)\n",
    "print(torch.tensor(input_ids))\n",
    "outputs = model.generate(input_ids=torch.tensor(input_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs)\n",
    "metrics = compute_metrics((outputs, label))\n",
    "\n",
    "\n",
    "bleu_score = metrics['bleu']\n",
    "meteor_score = metrics['meteor']\n",
    "ter_score = metrics['ter']\n",
    "print(\"BLEU Score on Test Example:\", bleu_score)\n",
    "print(\"METEOR Score on Test Dataset:\", meteor_score)\n",
    "print(\"TER Score on Test Dataset:\", ter_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "996e85f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Bleu Score:  14.597\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.predict(tokenized_datasets[\"test\"])\n",
    "\n",
    "\n",
    "print(\"Test Bleu Score: \", test_results.metrics[\"test_bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee0b5611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:  {'test_loss': 2.9722914695739746, 'test_bleu': 14.597, 'test_meteor': 0.3538, 'test_ter': 74.4472, 'test_gen_len': 9.92730007520682, 'test_runtime': 911.4809, 'test_samples_per_second': 4.376, 'test_steps_per_second': 0.274}\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Metrics: \", test_results.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
